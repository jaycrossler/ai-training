{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaycrossler/ai-training/blob/main/Mistral%20Small%20via%20API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2e00a27-64ef-4431-967e-940768fda8fa",
      "metadata": {
        "id": "f2e00a27-64ef-4431-967e-940768fda8fa"
      },
      "source": [
        "# Getting started with Mistral Small via API\n",
        "\n",
        "Based on a colab [notebook](https://colab.research.google.com/drive/1zo794DObJNXwj3ohJHx6dI0ALUV0rJYu?usp=sharing#scrollTo=rVJii8iRQG65) from Sam Witteveen - checkout [his YouTube channel](https://www.youtube.com/watch?v=nCXTdcggwkM) for more details.\n",
        "\n",
        "Requires a free MISTRAL_API_KEY that requires a signup from https://console.mistral.ai/api-keys/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e0eb939e-a7e6-42d9-a7ce-c61444c5dc62",
      "metadata": {
        "id": "e0eb939e-a7e6-42d9-a7ce-c61444c5dc62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbff2c34-d7dc-425a-b73c-f8062754547b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/271.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m266.2/271.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip -q install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')"
      ],
      "metadata": {
        "id": "pDoebnf4ca0f"
      },
      "id": "pDoebnf4ca0f",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(\"Mistral API Key setup complete.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aU2BV2hLcRcr",
        "outputId": "d0eb1cc0-a754-4e7a-a4dd-06d0a4ae7b5d"
      },
      "id": "aU2BV2hLcRcr",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Mistral API Key setup complete."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b630861-a95e-4925-8525-d9461d3627ea",
      "metadata": {
        "id": "5b630861-a95e-4925-8525-d9461d3627ea"
      },
      "source": [
        "Our API is currently available through [La Plateforme](https://console.mistral.ai/). You need to activate payments on your account to enable your API keys. After a few moments, you will be able to use our `chat` endpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2868793c-9065-459a-9a8e-214c31b98f1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2868793c-9065-459a-9a8e-214c31b98f1d",
        "outputId": "d0f1571b-d410-4b0a-8528-e3c09c78e56c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choosing the \"best\" Air Force career field depends on your interests, skills, and personal goals. Here are a few highly regarded career fields in the Air Force, each with its own unique aspects:\n",
            "\n",
            "1. **Pilot**:\n",
            "   - **Pros**: Flying advanced aircraft, leading missions, and experiencing a unique lifestyle.\n",
            "   - **Cons**: High stress, rigorous training, and potential for frequent relocations.\n",
            "\n",
            "2. **Cyber Warfare Operations**:\n",
            "   - **Pros**: High demand for skills, opportunities for cutting-edge technology work, and critical mission roles.\n",
            "   - **Cons**: High levels of stress, long hours, and continuous learning requirements.\n",
            "\n",
            "3. **Special Operations**:\n",
            "   - **Pros**: Highly specialized training, elite status, and dynamic missions.\n",
            "   - **Cons**: High physical and mental demands, dangerous missions, and rigorous selection processes.\n",
            "\n",
            "4. **Intelligence**:\n",
            "   - **Pros**: Analytical work, critical mission support, and opportunities for advanced education.\n",
            "   - **Cons**: Often involves long hours, classified work, and high stress.\n",
            "\n",
            "5. **Medical and Dental**:\n",
            "   - **Pros**: Directly helping people, opportunities for advanced training, and stable career paths.\n",
            "   - **Cons**: High stress, long hours, and potential for working in combat zones.\n",
            "\n",
            "6. **Engineering and Maintenance**:\n",
            "   - **Pros**: Hands-on work with advanced technology, opportunities for problem-solving, and varied roles.\n",
            "   - **Cons**: Physically demanding work, potential for hazardous conditions, and long hours.\n",
            "\n",
            "7. **Space Operations**:\n",
            "   - **Pros**: Exciting and cutting-edge technology, high demand, and unique mission roles.\n",
            "   - **Cons**: Long training periods, high stress, and potential for frequent relocations.\n",
            "\n",
            "8. **Cyber Operations and IT**:\n",
            "   - **Pros**: High demand for skills, opportunities for advanced education, and critical mission roles.\n",
            "   - **Cons**: High levels of stress, long hours, and continuous learning requirements.\n",
            "\n",
            "9. **Fighter Pilots**:\n",
            "   - **Pros**: High excitement, prestige, and performing elite missions.\n",
            "   - **Cons**: High physical and mental demands, rigorous training, and dangerous missions.\n",
            "\n",
            "10. **Nursing**:\n",
            "    - **Pros**: Directly helping people, opportunities for advanced training, and stable career paths.\n",
            "    - **Cons**: High stress, long hours, and potential for working in combat zones.\n",
            "\n",
            "Ultimately, the \"best\" career field is the one that aligns with your personal interests, skills, and long-term goals. It's also important to consider the lifestyle, training requirements, and potential for career growth in each field.\n"
          ]
        }
      ],
      "source": [
        "from mistralai import Mistral\n",
        "\n",
        "\n",
        "model = \"mistral-small-2501\" #mistral-small-2501\n",
        "# model = \"mistral-small-latest\"\n",
        "\n",
        "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=[{\"role\":\"user\", \"content\":\"What is the best Air Force career field?\"}]\n",
        ")\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using it with LangChain"
      ],
      "metadata": {
        "id": "ILX4tqZ0dXCo"
      },
      "id": "ILX4tqZ0dXCo"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U langchain-core langchain-mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYbUiaHDcHXm",
        "outputId": "d70b2cf9-0c8e-46e7-8f0c-7d5a6e627bbd"
      },
      "id": "dYbUiaHDcHXm",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/412.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "Z3_u-8NmYnEV"
      },
      "execution_count": 7,
      "outputs": [],
      "id": "Z3_u-8NmYnEV"
    },
    {
      "cell_type": "code",
      "source": [
        "# If mistral_api_key is not passed, default behavior is to use the `MISTRAL_API_KEY` environment variable.\n",
        "chat = ChatMistralAI(\n",
        "    model=\"mistral-small-2501\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=128,\n",
        "                     )"
      ],
      "metadata": {
        "id": "NTxuyS99ajxA"
      },
      "execution_count": 8,
      "outputs": [],
      "id": "NTxuyS99ajxA"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content='Where does the shout \"huah\" come from?')]\n",
        "response = chat.invoke(messages)\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "-HROrBJCannn",
        "outputId": "3b39a67b-1286-4557-944d-07a62512d1bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The shout \"huah\" is often associated with military and paramilitary contexts, particularly in the United States. It is commonly used as a command or response in drills and exercises to signify readiness, acknowledgment, or execution of a task. The origin of the term \"huah\" is not definitively documented, but it is believed to have emerged from military training practices.\n\nHere are a few possible origins:\n\n1. **Training Calls**: The term might have arisen as a distinctive sound that could be easily recognized and replicated by trainees, similar to other military cadences and commands designed to ensure uniformity and clarity.\n\n2. **"
          },
          "metadata": {}
        }
      ],
      "id": "-HROrBJCannn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model"
      ],
      "metadata": {
        "id": "AH98nvXdmGs3"
      },
      "id": "AH98nvXdmGs3"
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "pGeA9IlLh66S"
      },
      "execution_count": 10,
      "outputs": [],
      "id": "pGeA9IlLh66S"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E4mjX62HFPIf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate(input_text, system_prompt=\"\",max_length=512):\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=input_text)\n",
        "        ]\n",
        "    chat = ChatMistralAI(\n",
        "        model=\"mistral-small-2501\",\n",
        "        temperature=0.7,\n",
        "        max_tokens=max_length,\n",
        "                     )\n",
        "    response = chat.invoke(messages)\n",
        "    text = response.content\n",
        "    wrapped_text = wrap_text(text)\n",
        "    display(Markdown(wrapped_text))"
      ],
      "id": "E4mjX62HFPIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nhYTllvLTuF"
      },
      "source": [
        "## Instruction Answering"
      ],
      "id": "_nhYTllvLTuF"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3hoon3WAFeMd",
        "outputId": "799582ad-48b6-4942-962d-518f30cd5014"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, let's break down the analogy between systems engineering and football step-by-step:\n\n### 1. **Team Structure and Roles**\n\n**Football:**\n- A football team consists of various positions: quarterbacks, running backs, wide\nreceivers, offensive linemen, defensive linemen, linebackers, defensive backs, and special\nteams.\n- Each position has a specific role and responsibility, and all work together to win the\ngame.\n\n**Systems Engineering:**\n- A systems engineering project involves different roles: systems architects, software\nengineers, hardware engineers, project managers, quality assurance specialists, and test\nengineers.\n- Each role has a specific task and all roles collaborate to deliver a successful project.\n\n### 2. **Strategy and Planning**\n\n**Football:**\n- Before each game, the coaching staff develops a game plan, including strategies for\noffense, defense, and special teams.\n- Adjustments are made based on the opponent's strengths and weaknesses, and real-time\ndecisions are made during the game.\n\n**Systems Engineering:**\n- Planning is crucial in systems engineering. This includes defining requirements,\ndesigning the system architecture, and creating detailed project plans.\n- Systems engineers must adapt to changes in requirements, technological advancements, and\nproject constraints.\n\n### 3. **Coordination and Communication**\n\n**Football:**\n- Effective communication and coordination among players and coaching staff are essential.\n- Play calls, signals, and huddles ensure that everyone is on the same page and executing\nthe strategy correctly.\n\n**Systems Engineering:**\n- Clear communication and coordination among team members are vital.\n- Meetings, documentation, and collaborative tools ensure that everyone understands the\nproject goals, timelines, and deliverables.\n\n### 4. **Execution and Performance**\n\n**Football:**\n- During the game, players execute the game plan, make quick decisions, and adapt to\nchanging conditions on the field.\n- Performance is measured by the team's ability to score points, prevent the opponent from\nscoring, and win the game.\n\n**Systems Engineering:**\n- Execution involves implementing the design, coding, testing, and integrating various\ncomponents.\n- Performance is measured by the system's functionality, reliability, efficiency, and user\nsatisfaction.\n\n### 5. **Evaluation and Feedback**\n\n**Football:**\n- After each game, the coaching staff evaluates performance, identifies areas for\nimprovement, and provides feedback to players.\n- This continuous improvement helps the team perform better in future games.\n\n**Systems Engineering:**\n- After each phase or milestone, the engineering team evaluates the project's progress,\nidentifies issues, and provides feedback.\n- This iterative process helps in improving the system's design, implementation, and\noverall quality.\n\n### 6. **Risk Management**\n\n**Football:**\n- Teams must anticipate and prepare for potential risks, such as injuries, unfavorable\nweather conditions, and unexpected opponent strategies.\n- Contingency plans and backup strategies are essential to mitigate these risks.\n\n**Systems Engineering:**\n- Systems engineers must identify and manage risks throughout the project lifecycle.\n- Risk management plans, contingency reserves, and fallback strategies help in mitigating\npotential issues.\n\n### 7. **Innovation and Adaptation**\n\n**Football:**\n- Successful teams continually innovate and adapt their strategies to stay competitive.\n- New formations, plays, and technologies are introduced to gain an edge over opponents.\n\n**Systems Engineering:**\n- Systems engineering involves continuous innovation and adaptation to new technologies\nand methodologies.\n- Staying updated with industry trends and best practices ensures the delivery of cutting-\nedge solutions.\n\n### Conclusion\n\nBoth systems engineering and football require a well-coordinated team, strategic planning,\neffective communication, continuous evaluation, risk management, and a willingness to\ninnovate. By understanding and leveraging these parallels, we can gain insights into best\npractices and approaches for both fields."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('Write a detailed analogy between systems engineering and football.',\n",
        "         system_prompt=\"You are a helpful creative small language model. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "id": "3hoon3WAFeMd"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TrwFTMCYFeOq",
        "outputId": "e5b70bad-2263-4003-e799-e61420218878"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To create a detailed analogy between business enhancement software and air force jets,\nwe'll break down the key aspects of each and compare them step-by-step.\n\n### Step 1: Define the Key Components\n\n**Air Force Jets:**\n1. **Purpose:** To provide air superiority, conduct reconnaissance, and support ground\noperations.\n2. **Components:** Engines, avionics, weapons systems, communication systems, and\nstructural integrity.\n3. **Performance Metrics:** Speed, maneuverability, range, and payload capacity.\n4. **Maintenance:** Regular checks, updates, and upgrades to ensure optimal performance.\n5. **Operational Efficiency:** Ability to perform missions effectively with minimal\ndowntime.\n\n**Business Enhancement Software:**\n1. **Purpose:** To improve business operations, increase efficiency, and drive growth.\n2. **Components:** Modules for CRM (Customer Relationship Management), ERP (Enterprise\nResource Planning), Analytics, and Automated Workflows.\n3. **Performance Metrics:** Speed of data processing, user experience, integration\ncapabilities, and scalability.\n4. **Maintenance:** Regular updates, bug fixes, and feature enhancements.\n5. **Operational Efficiency:** Ability to streamline processes and reduce operational\ncosts.\n\n### Step 2: Compare the Components\n\n**Engines (Air Force Jets) vs. Core Modules (Business Enhancement Software):**\n- **Engines** provide the power and propulsion needed for jets to fly and perform their\nmissions. Similarly, **Core Modules** in business software provide the foundational\nfunctionality that drives business operations, such as CRM and ERP.\n\n**Avionics (Air Force Jets) vs. Analytics and Reporting (Business Enhancement Software):**\n- **Avionics** include navigation, communication, and control systems that help pilots\noperate the jet effectively. Similarly, **Analytics and Reporting** in business software\nprovide insights and data-driven decision-making capabilities, helping businesses navigate\ntheir operations.\n\n**Weapons Systems (Air Force Jets) vs. Automated Workflows (Business Enhancement\nSoftware):**\n- **Weapons Systems** are used to execute missions and defend against threats. Similarly,\n**Automated Workflows** in business software automate repetitive tasks, freeing up\nresources to focus on strategic activities and defend against operational inefficiencies.\n\n**Communication Systems (Air Force Jets) vs. Integration Capabilities (Business\nEnhancement Software):**\n- **Communication Systems** ensure effective coordination between different units and with\nground control. Similarly, **Integration Capabilities** in business software enable\nseamless data exchange and coordination between different business functions and external\nsystems.\n\n**Structural Integrity (Air Force Jets) vs. Security and Compliance (Business Enhancement\nSoftware):**\n- **Structural Integrity** ensures the jet can withstand the stresses of flight and\ncombat. Similarly, **Security and Compliance** in business software ensure that data is\nprotected and the system adheres to regulatory requirements, maintaining the integrity of\nbusiness operations.\n\n### Step 3: Compare Performance Metrics\n\n**Speed (Air Force Jets) vs. Processing Speed (Business Enhancement Software):**\n- **Speed** in jets is crucial for quick responses and maneuverability. Similarly,\n**Processing Speed** in business software is essential for quick data retrieval and\nanalysis, enabling timely decision-making.\n\n**Maneuverability (Air Force Jets) vs. Flexibility (Business Enhancement Software):**\n- **Maneuverability** allows jets to adapt to changing situations. Similarly,\n**Flexibility** in business software enables businesses to adapt to changing market\nconditions and customer needs.\n\n**Range (Air Force Jets) vs. Scalability (Business Enhancement Software):**\n- **Range** determines how far a jet can travel without refueling. Similarly,\n**Scalability** in business software determines how well it can handle increased data and\nuser loads as the business grows.\n\n**Payload Capacity (Air Force Jets) vs. Data Handling Capacity (Business Enhancement\nSoftware):**\n- **Payload Capacity** refers to the amount of equipment or cargo a jet can carry.\nSimilarly, **Data Handling Capacity** in business software refers to its ability to manage\nlarge volumes of data efficiently.\n\n### Step 4: Compare Maintenance and Operational Efficiency\n\n**Regular Checks and Updates (Air Force Jets) vs. Software Updates (Business Enhancement\nSoftware):**\n- Both require regular maintenance to ensure they are performing optimally and to address\nany issues that may arise. Regular checks and updates in jets ensure they are mission-\nready, while software updates in business enhancement software ensure it remains secure,\nefficient, and feature-rich.\n\n**Operational Efficiency (Both):**\n- The ability to perform their respective missions with minimal downtime is crucial. For\njets, this means being ready for deployment at a moment's notice. For business software,\nthis means ensuring that the system is always available and operational, minimizing\ndisruptions to business processes.\n\n### Conclusion\n\nThe analogy between business enhancement software and air force jets highlights how both\nsystems are designed to enhance performance, ensure operational efficiency, and adapt to\nchanging environments. Just as air force jets require robust engines, advanced"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 157 ms, sys: 8.97 ms, total: 166 ms\n",
            "Wall time: 8.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a detailed analogy between business enhancement software and air force jets.',\n",
        "         system_prompt=\"You are a helpful creative small language model. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "id": "TrwFTMCYFeOq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "-ymIE3SVTvyN",
        "outputId": "8c6ed096-9fd7-43d8-b455-5933e8a147bf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To understand the differences between a Llama, Vicuna and an Alpaca, let's break down\ntheir characteristics step-by-step:\n\n### 1. **Llama**\n- **Scientific Name**: *Lama glama*\n- **Origin**: Native to South America, primarily in the Andes mountains.\n- **Size**: Llamas are generally the largest of the three species.\n  - Height: 4 to 5 feet (1.2 to 1.5 meters) at the shoulder.\n- **Weight**: 280 to 450 pounds (127 to 204 kilograms).\n- **Appearance**: Known for their long necks and large, banana-shaped ears.\n- **Behavior**: Often used as pack animals due to their strength and endurance. They are\nalso kept as pets and for their wool.\n- **Wool**: The wool is softer and more luxurious than Alpaca wool.\n\n### 2. **Vicuña**\n- **Scientific Name**: *Vicugna vicugna*\n- **Origin**: Native to the Andes mountains, primarily in Peru, Bolivia, Chile, and\nArgentina.\n- **Size**: Vicuñas are the smallest of the three species.\n  - Height: 3 to 3.5 feet (0.9 to 1.1 meters) at the shoulder.\n- **Weight**: 80 to 110 pounds (36 to 50 kilograms).\n- **Appearance**: Known for their slender build and fine wool.\n- **Behavior**: Vicuñas are wild and not domesticated. They produce the finest and most\nvaluable wool of any domesticated animal.\n  - The wool is highly sought after and can be worth more than gold by weight.\n- **Conservation Status**: Vicuñas are protected by law in several countries due to their\nendangered status.\n\n### 3. **Alpaca**\n- **Scientific Name**: *Vicugna pacos*\n- **Origin**: Native to South America, primarily in the Andes mountains.\n- **Size**: Alpacas are smaller than llamas but larger than Vicuñas.\n  - Height: 3 to 3.5 feet (0.9 to 1.1 meters) at the shoulder.\n- **Weight**: 100 to 200 pounds (45 to 91 kilograms).\n- **Appearance**: Alpacas have a rounded body and a small, wedge-shaped head. They are\noften fluffy and come in a variety of colors.\n- **Behavior**: Known for their gentle nature and are often kept as pets. They are\nprimarily raised for their wool.\n  - Alpacas are friendly and social animals.\n  - Alpaca wool is known for its softness and warmth, though it is slightly coarser than\nVicuña wool.\n\n### Summary of Differences:\n\n- **Size**: Llamas are the largest, followed by Alpacas, and Vicuñas are the smallest.\n- **Behavior**: Llamas are used as pack animals, Vicuñas are wild and protected, and\nAlpacas are domesticated and kept for their wool.\n- **Wool**: Vicuña wool is the finest and most valuable, followed by Llama wool, and\nAlpaca wool is the coarsest of the three.\n\nBy understanding these differences, you can better appreciate the unique characteristics\nof each animal."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 162 ms, sys: 4.2 ms, total: 166 ms\n",
            "Wall time: 4.93 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
        "         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "id": "-ymIE3SVTvyN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "IzQRjct_prr8",
        "outputId": "59af5180-6934-4b91-f64a-d1beee05f1f9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Subject: Proposal to Open Source GPT-4\n\nDear Sam,\n\nI hope this email finds you well. I am writing to discuss the potential benefits of open-\nsourcing GPT-4. Here are some reasons why this could be a strategic move for us:\n\n1. **Community Innovation**: By open-sourcing GPT-4, we can harness the collective\nintelligence of the global developer community. This could lead to rapid innovation and\nthe development of new, creative use cases that we might not have considered.\n\n2. **Transparency and Trust**: Open-sourcing our model can increase transparency, which is\ncrucial for building trust with users and stakeholders. It can also help address concerns\nabout bias, fairness, and privacy.\n\n3. **Talent Attraction**: An open-source project of this magnitude can attract top talent\nwho are eager to contribute to and learn from the project. This can help us build a\nstronger team and foster a culture of collaboration.\n\n4. **Educational Value**: Open-sourcing GPT-4 can serve as a valuable educational resource\nfor students and researchers interested in AI and machine learning. It can help advance\nthe field and inspire the next generation of AI experts.\n\n5. **Market Leadership**: By open-sourcing GPT-4, we can set a new standard for AI\ndevelopment and cement our position as a leader in the field. This can also encourage\nother companies to follow suit, fostering a more open and collaborative AI ecosystem.\n\nI understand that there are potential challenges and risks to consider, such as the risk\nof misuse or unintended consequences. However, I believe that with the right safeguards\nand community guidelines in place, the benefits could outweigh the risks.\n\nI would be happy to discuss this idea further at your convenience. Thank you for\nconsidering this proposal.\n\nBest regards,\n\n[Your Name]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 219 ms, sys: 10 ms, total: 229 ms\n",
            "Wall time: 3.07 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are a helpful creative small language model. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "id": "IzQRjct_prr8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Tcukqu1d2BKX",
        "outputId": "e266db62-8086-4b5a-a66c-850fb611de13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi Mr. Sam,\n\nI am Freddy, I am a 5 year old little boy. I want to talk to you about GPT-4. I heard you\nare not going to open source it. I don't think that is a good idea because if you don't\nopen source it then the smart people in the world can't help you make it better for us and\nhelp us learn. I don't want the AI to end the world.\n\nI'm just a little kid, but I have big fears. I want to help the world be a better place\nand I want you to help with that too. If you open source GPT-4, then the AI will help us\nbe better instead of ending the world. I think you can do it!\n\nPlease open source GPT-4!\n\nYour friend,\nFreddy"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 147 ms, sys: 5.02 ms, total: 152 ms\n",
            "Wall time: 1.51 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child!\",\n",
        "         max_length=1024)"
      ],
      "id": "Tcukqu1d2BKX"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "gid3ul9A20W8",
        "outputId": "dd5ca32f-30c2-4561-b7d0-c79b23e85aa7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Subject: Urgent: Recommendation to Open Source GPT-4\n\nDear Sam,\n\nI hope this message finds you well. I am writing to you as a senior Air Force officer who\nhas a strong belief in the power of open-source technology and the dangers of regulatory\ncapture. I firmly believe that open-sourcing GPT-4 could have significant benefits for\nboth society and national security.\n\nFirstly, open-sourcing GPT-4 would allow for greater transparency and scrutiny. This would\nhelp to identify and mitigate potential biases, vulnerabilities, and unintended\nconsequences of the technology. It would also foster a more robust and secure AI\necosystem, as a diverse range of experts would be able to contribute to its improvement.\n\nSecondly, open-sourcing could help to prevent regulatory capture by the large technology\ncompanies that currently dominate the AI landscape. By making GPT-4 open-source, we could\nhelp to level the playing field and encourage innovation from a wider range of actors,\nincluding small startups, academic institutions, and non-profit organizations.\n\nThirdly, as a senior Air Force officer, I am acutely aware of the importance of\nmaintaining a competitive edge in AI for national security purposes. Open-sourcing GPT-4\nwould allow for greater collaboration and innovation within the defense community, as well\nas with our allies. It would also help to ensure that the United States remains a leader\nin AI technology.\n\nI understand that there may be concerns about the potential misuse of the technology, but\nI believe that these risks can be managed through responsible stewardship and governance.\nI would be happy to discuss this further and explore how we can work together to make\nGPT-4 open-source in a way that maximizes its benefits while minimizing its risks.\n\nBest regards,\n\n[Your Name]\nSenior Air Force Officer"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 152 ms, sys: 6.74 ms, total: 158 ms\n",
            "Wall time: 2.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are a senior Air Force officer, you are against regulatory capture and like to explain that!\",\n",
        "         max_length=1024)"
      ],
      "id": "gid3ul9A20W8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "YXZHQ0v3Tv0d",
        "outputId": "a7314563-afdb-43ae-e1e1-9f13ad57ea94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "London"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 160 ms, sys: 993 µs, total: 161 ms\n",
            "Wall time: 647 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('What is the capital of England?',\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your short and succinct answer!\",\n",
        "         max_length=256)"
      ],
      "id": "YXZHQ0v3Tv0d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "TnGbQ7iU0XDK",
        "outputId": "eda95074-d629-4d14-d7ab-2ae5751f991e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To determine if Geoffrey Hinton can have a conversation with George Washington, we need to\nconsider several factors:\n\n1. **Temporal Difference**: Geoffrey Hinton is a living person in the 21st century, while\nGeorge Washington lived from 1732 to 1799. This means there is a significant temporal gap\nof over two centuries between the two individuals.\n\n2. **Time Travel**: For a conversation to occur, one of them would need to travel through\ntime, which is currently beyond the capabilities of known technology or science. There is\nno scientifically proven method for time travel.\n\n3. **Communication Medium**: Even if time travel were possible, the two individuals would\nneed a common medium or technology to facilitate communication. The technological and\ncultural differences between their eras would pose significant challenges.\n\n4. **Historical Record**: There are no historical records or documented instances of\nGeorge Washington communicating with anyone from the future. Additionally, Geoffrey Hinton\nhas not claimed to have engaged in such a conversation.\n\nGiven these points, it is not possible for Geoffrey Hinton to have a conversation with\nGeorge Washington based on current scientific understanding and historical records.\n\n**Conclusion**: No, Geoffrey Hinton cannot have a conversation with George Washington."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 220 ms, sys: 2.12 ms, total: 223 ms\n",
            "Wall time: 2.29 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.',\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ],
      "id": "TnGbQ7iU0XDK"
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Write a two paragraph funny story about a an Air Force soldier beating a Navy and Army officer in a game.',\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral AI, a genius story teller. Write out your with details and make it compelling!\",\n",
        "         max_length=2048)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "_VJUGNUefhVf",
        "outputId": "9a6254e3-9e71-4c2b-e441-7a8d1f37b357"
      },
      "id": "_VJUGNUefhVf",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In the bustling recreation room of a joint military base, Air Force Sergeant \"Jet\"\nJackson, a lanky man with a mischievous grin, was challenged to a game of ping-pong by\nArmy Captain \"Tank\" Thompson and Navy Lieutenant \"Wave\" Walker. Both were confident in\ntheir victory, given their well-known prowess and the fact that Jet was known more for his\nlove of chess. However, Jet, a man who once used a broom to keep himself entertained and\nhoned his skills during a lonely night shift, accepted the challenge.\n\nWhat ensued was a display of ping-pong prowess that left the mess hall in a state of\ndisbelief. Jet, with a nonchalant smile and an uncanny ability to predict the ball's\ntrajectory, began to dominate the game. He utilized every trick in the book, from the\nclassic spin shots to the more advanced sideways shots and even a few stunts that made him\nlook like he had just come from a circus act. The final round ended with a flourish, as\nJet dramatically leaped into the air and hit the ball into the net, securing his victory.\nThe mess hall erupted in cheers, and Tank and Wave, despite their initial disbelief,\ncouldn't help but shake their heads and laugh, realizing that the Air Force wasn't just\nabout flying, but also about the unexpected."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZ_h-3iQf2IJ"
      },
      "id": "GZ_h-3iQf2IJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnLzgM_dQDVm"
      },
      "source": [
        "## GSM8K\n",
        "[GSM8K](https://huggingface.co/datasets/openai/gsm8k) (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.\n",
        "\n",
        "These problems take between 2 and 8 steps to solve."
      ],
      "id": "wnLzgM_dQDVm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "rVJii8iRQG65",
        "outputId": "1f33a5e6-7412-4fda-b205-1646c3bcf4af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's break this down step-by-step:\n\n1. The cafeteria started with 23 apples.\n2. They used 20 apples for lunch, so we subtract 20 from 23:\n   23 - 20 = 3 apples remaining.\n3. Then, they bought 6 more apples, so we add 6 to the remaining 3:\n   3 + 6 = 9 apples.\n\nTherefore, the cafeteria has 9 apples."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?',\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=256)"
      ],
      "id": "rVJii8iRQG65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "uM8iY879QJ66",
        "outputId": "9699ea9f-7496-41f2-fc34-0f851cc8d685"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To calculate Weng's earnings for 50 minutes of babysitting, we need to follow these steps:\n\n1. **Determine the hourly rate**: Weng earns $12 per hour.\n2. **Convert the time worked into hours**: Since she worked 50 minutes, we need to convert\nthis into hours. There are 60 minutes in an hour, so:\n   \\[\n   50 \\text{ minutes} = \\frac{50}{60} \\text{ hours} = \\frac{5}{6} \\text{ hours}\n   \\]\n3. **Calculate the earnings**: Multiply the hourly rate by the number of hours worked:\n   \\[\n   \\text{Earnings} = 12 \\times \\frac{5}{6} = 2 \\times 5 = 10\n   \\]\n\nTherefore, Weng earned $10 for 50 minutes of babysitting."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ],
      "id": "uM8iY879QJ66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "u-3V9dvQQezx",
        "outputId": "a0c3796d-3fc9-41bb-e1f9-666ef2c0c5de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To solve this problem, let's denote the number of people on the ship eaten in the first\nhundred years as P. Then, according to the problem, the number of people on the ships\neaten in the second and third hundred years would be 2P and 4P, respectively, because each\nnew ship has twice as many people as the last ship.\n\nThe total number of people consumed over three hundred years is given as 847. Therefore,\nwe can set up the following equation:\n\nP + 2P + 4P = 847\n\nCombining like terms, we get:\n\n7P = 847\n\nNow, we solve for P:\n\nP = 847 / 7\nP = 121\n\nSo, there were 121 people on the ship the monster ate in the first hundred years."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate(\"Answer the following question by reasoning step by step. A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\",\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ],
      "id": "u-3V9dvQQezx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "x6NNSgl5ekKu",
        "outputId": "10d01ebc-bb3d-4d39-86f1-94dfc97049c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To find the value of x, we need to combine like terms and then solve for x. Here are the\nsteps:\n\n1. Combine like terms:\n   x + 2x + 4x = 7x\n\n2. Set up the equation:\n   7x = 847\n\n3. Solve for x by dividing both sides by 7:\n   x = 847 / 7\n\n4. Calculate the division:\n   x = 121\n\nSo, x = 121."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate(\"x + 2x + 4x =  847 What is x?\",\n",
        "         system_prompt=\"You are Mistral Small, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=2048)"
      ],
      "id": "x6NNSgl5ekKu"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "\n",
        "model = ChatMistralAI(model=\"mistral-small-2501\")\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    name: str = Field(..., description=\"The name of the person\")\n",
        "    height_in_meters: float = Field(\n",
        "        ..., description=\"The height of the person expressed in meters.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class People(BaseModel):\n",
        "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
        "\n",
        "    people: List[Person]\n",
        "\n",
        "\n",
        "# Set up a parser\n",
        "parser = PydanticOutputParser(pydantic_object=People)\n",
        "\n",
        "# Prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
        "        ),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ").partial(format_instructions=parser.get_format_instructions())"
      ],
      "metadata": {
        "id": "hzdohsbrgE4z"
      },
      "id": "hzdohsbrgE4z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Anna is 23 years old and she is 6 feet tall\""
      ],
      "metadata": {
        "id": "FZZbfmlPiG5V"
      },
      "id": "FZZbfmlPiG5V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format_prompt(query=query).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh7L1Db4iKt2",
        "outputId": "1a0f322c-4df8-4a1f-a233-999fd2c91045"
      },
      "id": "Sh7L1Db4iKt2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Answer the user query. Wrap the output in `json` tags\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"$defs\": {\"Person\": {\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"description\": \"The name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"height_in_meters\": {\"description\": \"The height of the person expressed in meters.\", \"title\": \"Height In Meters\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"], \"title\": \"Person\", \"type\": \"object\"}}, \"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"items\": {\"$ref\": \"#/$defs/Person\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\n",
            "```\n",
            "Human: Anna is 23 years old and she is 6 feet tall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | parser\n",
        "chain.invoke({\"query\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blqUTRXCiexG",
        "outputId": "1b7428cb-d94f-486c-9f49-a4d21075cb38"
      },
      "id": "blqUTRXCiexG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "People(people=[Person(name='Anna', height_in_meters=1.8288)])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking Function calling"
      ],
      "metadata": {
        "id": "I7TOb0QQj2wp"
      },
      "id": "I7TOb0QQj2wp"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "class add(TypedDict):\n",
        "    \"\"\"Add two integers.\"\"\"\n",
        "\n",
        "    # Annotations must have the type and can optionally include a default value and description (in that order).\n",
        "    a: Annotated[int, ..., \"First integer\"]\n",
        "    b: Annotated[int, ..., \"Second integer\"]\n",
        "\n",
        "\n",
        "class multiply(TypedDict):\n",
        "    \"\"\"Multiply two integers.\"\"\"\n",
        "\n",
        "    a: Annotated[int, ..., \"First integer\"]\n",
        "    b: Annotated[int, ..., \"Second integer\"]\n",
        "\n",
        "\n",
        "tools = [add, multiply]"
      ],
      "metadata": {
        "id": "6NS5GaydiWZU"
      },
      "id": "6NS5GaydiWZU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "query = \"What is 3 * 128?\"\n",
        "\n",
        "llm_with_tools.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWK2CuXjj6CK",
        "outputId": "175598db-6a08-4a83-e2c0-96c7c398313d"
      },
      "id": "NWK2CuXjj6CK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '1s9Pi0jUO', 'function': {'name': 'multiply', 'arguments': '{\"a\": 3, \"b\": 128}'}, 'index': 0}]}, response_metadata={'token_usage': {'prompt_tokens': 176, 'total_tokens': 203, 'completion_tokens': 27}, 'model': 'mistral-small-2501', 'finish_reason': 'tool_calls'}, id='run-dc97f9af-5201-4bbc-afd9-370caefcc435-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 128}, 'id': '1s9Pi0jUO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 176, 'output_tokens': 27, 'total_tokens': 203})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        "\n",
        "llm_with_tools.invoke(query).tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN1pIA-Jj-Ph",
        "outputId": "3e27fa20-54c9-4134-8eed-a322b04640a5"
      },
      "id": "KN1pIA-Jj-Ph",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 3, 'b': 12},\n",
              "  'id': '5SrHRJ5qv',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'add',\n",
              "  'args': {'a': 11, 'b': 49},\n",
              "  'id': 'OkqR6y1WA',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWBxMD5ukVmm"
      },
      "id": "TWBxMD5ukVmm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
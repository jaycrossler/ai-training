{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Agentic RAG\n",
        "Agentic Retrieval-Augmented Generation (RAG) uses agents to verify the source of knowledge, hopefully increasing accuracy.\n",
        "\n",
        "Note, you will need an **OPENAI_API_KEY** loaded as a key (on the left if in Colab) and shared as secrets.\n",
        "\n",
        "This will also **build a hosted web app** (using Gradio) that others can use."
      ],
      "metadata": {
        "id": "MT4Z9xRQVUM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blog\n",
        "\n",
        "For a detailed explanation of agentic rag, check out  [blog post on Medium](https://aksdesai1998.medium.com/662bac582da9) and original code from https://github.com/lancedb/vectordb-recipes/tree/main/tutorials/Agentic_RAG.\n"
      ],
      "metadata": {
        "id": "dbzbP8U3WDov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mstdPht-oONc"
      },
      "outputs": [],
      "source": [
        "# install the required dependencies\n",
        "%%capture --no-stderr\n",
        "%pip install -U --quiet langchain-community tiktoken langchain-openai langchainhub lancedb  langchain langgraph langchain-text-splitters langchain_openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rieiaeu9posq",
        "outputId": "31d05a94-c84a-4f68-fd46-ed9f43b638ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "import gradio as gr\n",
        "from typing import Annotated, Literal, Sequence, TypedDict\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import LanceDB\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolExecutor, ToolNode, tools_condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpwx86tgorLG",
        "outputId": "ddaa7395-e59e-4463-ca75-823c20c64ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b5321f1673b3>:49: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
            "  tool_executor = ToolExecutor(tools)\n"
          ]
        }
      ],
      "source": [
        "# Function to set environment variables securely\n",
        "def _set_env(key: str):\n",
        "    if key not in os.environ:\n",
        "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# (Optional) For tracing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"False\"\n",
        "#_set_env(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "\n",
        "# upload the data based on your usecase\n",
        "\n",
        "urls = [\n",
        "    \"https://www.doctrine.af.mil/Portals/61/documents/AFDP_1/AFDP-1.pdf\",\n",
        "    \"https://www.doctrine.af.mil/Portals/61/documents/AFDP_3-12/3-12-AFDP-CYBERSPACE-OPS.pdf\",\n",
        "#    \"https://www.doctrine.af.mil/Portals/61/documents/AFDP_3-13/3-13-AFDP-INFO-OPS.pdf\",\n",
        "]\n",
        "\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100, chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add to  lancedb as vectordb\n",
        "\n",
        "vectorstore = LanceDB.from_documents(\n",
        "    documents=doc_splits,\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "I5tpf0rOk4lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the tools\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_blog_posts\",\n",
        "    \"Search and return information about Air Force Cybersecurity doctrine\",\n",
        ")\n",
        "\n",
        "tools = [retriever_tool]\n",
        "tool_executor = ToolExecutor(tools)\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "\n",
        "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
        "    class grade(BaseModel):\n",
        "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
        "\n",
        "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
        "    llm_with_tool = model.with_structured_output(grade)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
        "        Here is the user question: {question} \\n\n",
        "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "    )\n",
        "    chain = prompt | llm_with_tool\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    question = messages[0].content\n",
        "    docs = last_message.content\n",
        "\n",
        "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
        "    score = scored_result.binary_score\n",
        "\n",
        "    return \"generate\" if score == \"yes\" else \"rewrite\"\n",
        "\n",
        "\n",
        "def agent(state):\n",
        "    messages = state[\"messages\"]\n",
        "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\n",
        "    model = model.bind_tools(tools)\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def rewrite(state):\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    msg = [\n",
        "        HumanMessage(\n",
        "            content=f\"\"\" \\n\n",
        "            Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
        "            Here is the initial question:\n",
        "            \\n ------- \\n\n",
        "            {question}\n",
        "            \\n ------- \\n\n",
        "            Formulate an improved question: \"\"\",\n",
        "        )\n",
        "    ]\n",
        "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
        "    response = model.invoke(msg)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    last_message = messages[-1]\n",
        "    docs = last_message.content\n",
        "\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"agent\", agent)\n",
        "retrieve = ToolNode([retriever_tool])\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"rewrite\", rewrite)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\", tools_condition, {\"tools\": \"retrieve\", END: END}\n",
        ")\n",
        "workflow.add_conditional_edges(\"retrieve\", grade_documents)\n",
        "workflow.add_edge(\"generate\", END)\n",
        "workflow.add_edge(\"rewrite\", \"agent\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "\n",
        "def process_message(user_message):\n",
        "    inputs = {\"messages\": [(\"user\", user_message)]}\n",
        "    content_output = None\n",
        "    for output in graph.stream(inputs):\n",
        "        print(f\"Debug output: {output}\")  # Debugging line to print the output\n",
        "        if \"agent\" in output and \"messages\" in output[\"agent\"]:\n",
        "            messages = output[\"agent\"][\"messages\"]\n",
        "            if messages and hasattr(messages[0], \"content\"):\n",
        "                content_output = messages[0].content  # Accessing attribute directly\n",
        "                print(f\"Extracted content: {content_output}\")  # Print extracted content\n",
        "    return content_output if content_output else \"No relevant output found.\"\n",
        "\n",
        "\n",
        "# Define example questions to guide the user\n",
        "example_questions = [\n",
        "    \"Summarize what the rules are for defending Air Force cyberspace and attacks on AI systems?\"\n",
        "]"
      ],
      "metadata": {
        "id": "MujevMNmkwzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_message,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Agentic RAG \",\n",
        "    description=\"Enter a message to query related to Air Force cyberspace and attacks on AI systems.\",\n",
        "    examples=example_questions,\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "mDtPDsgHjKz2",
        "outputId": "71b8fdfa-4663-4332-9884-eef94459563d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9ea5b3c1b868414a84.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9ea5b3c1b868414a84.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug output: {'agent': {'messages': [AIMessage(content='The operation conducted by Joint Task Force Ares against ISIS was named \"Operation Glowing Symphony.\" This operation targeted ISIS\\'s digital infrastructure and aimed to disrupt their online communications and propaganda efforts.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4-turbo-2024-04-09', 'system_fingerprint': 'fp_f17929ee92'}, id='run-fecb202b-e103-464e-ae37-c7c8583fb7d3-0')]}}\n",
            "Extracted content: The operation conducted by Joint Task Force Ares against ISIS was named \"Operation Glowing Symphony.\" This operation targeted ISIS's digital infrastructure and aimed to disrupt their online communications and propaganda efforts.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9ea5b3c1b868414a84.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaycrossler/ai-training/blob/main/Using%20AI%20Agents%20to%20build%20a%20Knowledge%20Graph%20and%20verify%20results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agentic RAG\n",
        "Agentic Retrieval-Augmented Generation (RAG) uses agents to verify the source of knowledge, hopefully increasing accuracy.\n",
        "\n",
        "Note, you will need an **OPENAI_API_KEY** loaded as a key (on the left if in Colab) and shared as secrets.\n",
        "\n",
        "This will also **build a hosted web app** (using Gradio) that others can use."
      ],
      "metadata": {
        "id": "MT4Z9xRQVUM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blog\n",
        "\n",
        "For a detailed explanation of agentic rag, check out  [blog post on Medium](https://aksdesai1998.medium.com/662bac582da9) and original code from https://github.com/lancedb/vectordb-recipes/tree/main/tutorials/Agentic_RAG.\n"
      ],
      "metadata": {
        "id": "dbzbP8U3WDov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mstdPht-oONc"
      },
      "outputs": [],
      "source": [
        "# install the required dependencies\n",
        "%%capture --no-stderr\n",
        "%pip install -U --quiet langchain-community tiktoken langchain-openai langchainhub lancedb  langchain langgraph langchain-text-splitters langchain_openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rieiaeu9posq",
        "outputId": "47978027-abf8-4104-d3c3-78ac35353f17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "import gradio as gr\n",
        "from typing import Annotated, Literal, Sequence, TypedDict\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import LanceDB\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolExecutor, ToolNode, tools_condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wpwx86tgorLG"
      },
      "outputs": [],
      "source": [
        "#Loads the URLs and Splits text into chunks - takes about 1 minute\n",
        "\n",
        "# Function to set environment variables securely\n",
        "def _set_env(key: str):\n",
        "    if key not in os.environ:\n",
        "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# (Optional) For tracing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"False\"\n",
        "#_set_env(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "\n",
        "# upload urls to sources based on your use case\n",
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/United_States_Air_Force\", # Do we need this? The model most likely knows this\n",
        "    \"https://www.doctrine.af.mil/Portals/61/documents/AFDP_1/AFDP-1.pdf\",\n",
        "    \"https://www.doctrine.af.mil/Portals/61/documents/AFDP_3-12/3-12-AFDP-CYBERSPACE-OPS.pdf\",\n",
        "]\n",
        "\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100, chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the split content to lancedb as knowledge vectors - takes about 3.5 minutes\n",
        "\n",
        "vectorstore = LanceDB.from_documents(\n",
        "    documents=doc_splits,\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "I5tpf0rOk4lm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the knowledge store has content\n",
        "\n",
        "table_vectors = vectorstore.get_table()\n",
        "print(table_vectors.count_rows())\n",
        "print(table_vectors.to_pandas())"
      ],
      "metadata": {
        "id": "CEx2pGdoqNzx",
        "outputId": "d62ce6a2-af0c-475c-92fc-57e9bb1437ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36087\n",
            "                                              vector  \\\n",
            "0  [-0.0070624077, -0.013837619, -0.00607354, -0....   \n",
            "1  [0.011888713, 0.005399457, -0.005541461, -0.02...   \n",
            "2  [0.015158966, 0.00059414015, 0.0043268176, -0....   \n",
            "3  [0.0219406, -0.0118596, -0.0015316671, -0.0137...   \n",
            "4  [0.002864139, -0.012553102, -0.015122923, -0.0...   \n",
            "5  [-0.010749912, -0.0048866277, -0.0021539961, -...   \n",
            "6  [-0.0022778655, -0.013432504, 0.014647366, -0....   \n",
            "7  [-0.0070134318, -0.008831483, 0.008698778, -0....   \n",
            "8  [-0.010623703, -0.005616577, -0.014177768, -0....   \n",
            "9  [-0.015056208, -0.013588921, -0.021031104, -0....   \n",
            "\n",
            "                                     id  \\\n",
            "0  e2e9bf92-9ab9-47d6-adf5-514b2e1d615e   \n",
            "1  f00910d1-3f1e-4474-9b15-d4450c7fcd3f   \n",
            "2  f78c5df5-f374-4c6f-a66c-e82997e97397   \n",
            "3  ffa19483-13bc-4917-bb83-cd77585da608   \n",
            "4  0dd067d4-28d8-4b3d-a624-df9330e33986   \n",
            "5  8e8bb5cc-78b1-4631-9b68-6e41bbed83d8   \n",
            "6  fa7d77df-bb17-4bd2-a52c-ab28387cadad   \n",
            "7  8cc31d1c-05dd-4845-8877-debf6733404a   \n",
            "8  4eb04f73-1758-40d4-b88c-11aa54a8093e   \n",
            "9  cb3b1366-8b84-4f7f-91e1-f024c33f079f   \n",
            "\n",
            "                                                text  \\\n",
            "0  United States Air Force - Wikipedia\\n\\n\\n\\n\\n\\...   \n",
            "1  Navigation\\n\\t\\n\\n\\nMain pageContentsCurrent e...   \n",
            "2  Search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n...   \n",
            "3  Personal tools\\n\\n\\n\\n\\n\\nDonate Create accoun...   \n",
            "4  (Top)\\n\\n\\n\\n\\n\\n1\\nMission, vision, and funct...   \n",
            "5  1.2.1\\nAir superiority\\n\\n\\n\\n\\n\\n\\n\\n\\n1.2.2\\...   \n",
            "6  1.2.5\\nCommand and control\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
            "7  2.3\\nConflicts\\n\\n\\n\\n\\n\\n\\n\\n\\n2.4\\nHumanitar...   \n",
            "8  4.1\\nAdministrative organization\\n\\n\\n\\n\\n\\n\\n...   \n",
            "9  4.4.1\\nAir Expeditionary Task Force\\n\\n\\n\\n\\n\\...   \n",
            "\n",
            "                                            metadata  \n",
            "0  {'language': 'en', 'source': 'https://en.wikip...  \n",
            "1  {'language': 'en', 'source': 'https://en.wikip...  \n",
            "2  {'language': 'en', 'source': 'https://en.wikip...  \n",
            "3  {'language': 'en', 'source': 'https://en.wikip...  \n",
            "4  {'language': 'en', 'source': 'https://en.wikip...  \n",
            "5  {'language': 'en', 'source': 'https://en.wikip...  \n",
            "6  {'language': 'en', 'source': 'https://en.wikip...  \n",
            "7  {'language': 'en', 'source': 'https://en.wikip...  \n",
            "8  {'language': 'en', 'source': 'https://en.wikip...  \n",
            "9  {'language': 'en', 'source': 'https://en.wikip...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the tools\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_blog_posts\",\n",
        "    \"Search and return information about Air Force Cybersecurity doctrine\",\n",
        ")\n",
        "\n",
        "tools = [retriever_tool]\n",
        "tool_executor = ToolExecutor(tools)\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "\n",
        "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
        "    class grade(BaseModel):\n",
        "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
        "\n",
        "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
        "    llm_with_tool = model.with_structured_output(grade)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
        "        Here is the user question: {question} \\n\n",
        "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "    )\n",
        "    chain = prompt | llm_with_tool\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    question = messages[0].content\n",
        "    docs = last_message.content\n",
        "\n",
        "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
        "    score = scored_result.binary_score\n",
        "\n",
        "    return \"generate\" if score == \"yes\" else \"rewrite\"\n",
        "\n",
        "\n",
        "def agent(state):\n",
        "    messages = state[\"messages\"]\n",
        "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\n",
        "    model = model.bind_tools(tools)\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def rewrite(state):\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    msg = [\n",
        "        HumanMessage(\n",
        "            content=f\"\"\" \\n\n",
        "            Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
        "            Here is the initial question:\n",
        "            \\n ------- \\n\n",
        "            {question}\n",
        "            \\n ------- \\n\n",
        "            Formulate an improved question: \"\"\",\n",
        "        )\n",
        "    ]\n",
        "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
        "    response = model.invoke(msg)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    last_message = messages[-1]\n",
        "    docs = last_message.content\n",
        "\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"agent\", agent)\n",
        "retrieve = ToolNode([retriever_tool])\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"rewrite\", rewrite)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\", tools_condition, {\"tools\": \"retrieve\", END: END}\n",
        ")\n",
        "workflow.add_conditional_edges(\"retrieve\", grade_documents)\n",
        "workflow.add_edge(\"generate\", END)\n",
        "workflow.add_edge(\"rewrite\", \"agent\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "\n",
        "def process_message(user_message):\n",
        "    inputs = {\"messages\": [(\"user\", user_message)]}\n",
        "    content_output = None\n",
        "    for output in graph.stream(inputs):\n",
        "        print(f\"Debug output: {output}\")  # Debugging line to print the output\n",
        "        if \"agent\" in output and \"messages\" in output[\"agent\"]:\n",
        "            messages = output[\"agent\"][\"messages\"]\n",
        "            if messages and hasattr(messages[0], \"content\"):\n",
        "                content_output = messages[0].content  # Accessing attribute directly\n",
        "                print(f\"Extracted content: {content_output}\")  # Print extracted content\n",
        "    return content_output if content_output else \"No relevant output found.\"\n",
        "\n",
        "\n",
        "# Define example questions to guide the user\n",
        "example_questions = [\n",
        "    \"Summarize what the rules are for defending Air Force cyberspace and attacks on AI systems?\",\n",
        "    \"What MAJCOM is at Wright-Patterson?\",\n",
        "    \"Which Operation did Joint Task Force Ares conduct against ISIS?\",\n",
        "    \"What does the 616 OC do?\"\n",
        "]"
      ],
      "metadata": {
        "id": "MujevMNmkwzu",
        "outputId": "67a4b262-5dca-4ee5-ce4d-5aab84ac5cee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-e0dd79ed5d36>:9: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
            "  tool_executor = ToolExecutor(tools)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_message,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Searching PDFs and rewriting results using agents\",\n",
        "    description=\"Enter a message to query related to Air Force cyberspace and attacks on AI systems.\",\n",
        "    examples=example_questions,\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "mDtPDsgHjKz2",
        "outputId": "7ad3bce3-2a28-4653-a01e-20059695d06a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://200ba03cd1d6d337dc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://200ba03cd1d6d337dc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
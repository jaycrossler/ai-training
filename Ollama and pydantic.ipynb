{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaycrossler/ai-training/blob/main/Ollama%20and%20pydantic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNRXnKgMErcy"
      },
      "source": [
        "# LLM Outputs from pydantic-ai\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB8-Nl94Ew8v"
      },
      "source": [
        "## Step 1: Create a LLM server with ollama\n",
        "\n",
        "To run this notebook, we need to have a OpenAI Compatible server. You can connect you own OpenAI account, huggingface CLI or use a local server. In the next cell, we will create an LLM server running on colab so that you dont' need to use any of the prior options.\n",
        "> Note: If you are running this code on the Google Colab, be sure to check if you have a GPU (Runtime menu->`Change runtime type`->`gpu T4`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfPJ3rM8mrs1",
        "outputId": "bf5b2002-bfcc-4f47-a0ed-8823c42afcab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Download and install Ollama which will serve the LLM\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0H5uSrMntRAS"
      },
      "outputs": [],
      "source": [
        "# Importing nesseracy libraries\n",
        "import subprocess\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "UQuNiabfNlmW"
      },
      "outputs": [],
      "source": [
        "# Start ollama in the background and use llama3.1 model\n",
        "\n",
        "# Start the process in the background\n",
        "server = subprocess.Popen(['ollama', 'serve'])\n",
        "time.sleep(60) # To make sure ollama is ready in subsequent cell if you are running all not cell at a time\n",
        "\n",
        "# To kill the server\n",
        "# server.kill()\n",
        "\n",
        "# To see all the models available: https://ollama.com/library\n",
        "# Note: llama3.3 takes about 10 minutes to install and is slow to run (need more ram)\n",
        "# Note: phi-4 and deepseek don't support tools in the pydantic format, so 3.2 seems best\n",
        "MODEL = 'llama3.2'\n",
        "llama3 = subprocess.Popen(['ollama', 'run', MODEL])\n",
        "#time.sleep(90) # Make sure ollama is ready in subsequent cell if you are running all not cell at a time\n",
        "\n",
        "# To kill the llama3\n",
        "# llama3.kill()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.Popen(['ollama', 'run', 'phi4'])"
      ],
      "metadata": {
        "id": "9FtXyhQSp3GP",
        "outputId": "33245c37-f70b-4cfd-aafe-99e10cc6128a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['ollama', 'run', 'phi4']>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show which model(s) ollama is serving\n",
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-idUacltj7g",
        "outputId": "a8ca688e-83a4-47bf-a0d0-f1bf742e347c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                  ID              SIZE      MODIFIED          \n",
            "phi4:latest           ac896e5b8b34    9.1 GB    2 minutes ago        \n",
            "llama3.3:latest       a6eb4748fd29    42 GB     2 minutes ago        \n",
            "deepseek-r1:latest    0a8c26691023    4.7 GB    3 minutes ago        \n",
            "llama3.2:latest       a80c4f17acd5    2.0 GB    11 minutes ago       \n",
            "llama3.1:latest       46e0c10c039e    4.9 GB    About an hour ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic-ai\n"
      ],
      "metadata": {
        "id": "AhXvhFW8dAK_",
        "outputId": "14e24db2-a1ab-4016-8844-f2f0093cbedd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic-ai in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: pydantic-ai-slim==0.0.20 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.0.20)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.2.2)\n",
            "Requirement already satisfied: griffe>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.5.5)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.28.1)\n",
            "Requirement already satisfied: logfire-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (3.3.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (2.10.5)\n",
            "Requirement already satisfied: anthropic>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.45.0)\n",
            "Requirement already satisfied: cohere>=5.13.11 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (5.13.11)\n",
            "Requirement already satisfied: pydantic-graph==0.0.20 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.0.20)\n",
            "Requirement already satisfied: groq>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.15.0)\n",
            "Requirement already satisfied: mistralai>=1.2.5 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.4.0)\n",
            "Requirement already satisfied: openai>=1.59.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.59.9)\n",
            "Requirement already satisfied: google-auth>=2.36.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (2.38.0)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (2.32.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (4.12.2)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.10.0)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (2.27.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.21.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (2.32.0.20241016)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (4.9)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe>=1.3.2->pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.4.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.14.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (2.8.2)\n",
            "Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.9.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.59.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->pydantic-ai-slim==0.0.20->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (2.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (0.27.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.9.0->mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (1.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.20->pydantic-ai) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent\n",
        "\n",
        "agent = Agent(\n",
        "    'ollama:llama3.2'\n",
        "    #system_prompt='Be concise, reply with one sentence.',\n",
        ")\n",
        "\n",
        "result = await agent.run('Where were the olympics held in 2012 ?')\n",
        "print(result.data)"
      ],
      "metadata": {
        "id": "allcLrA9dI4U",
        "outputId": "a67b5138-6038-4a4e-efe8-9e2a009fc5e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Olympics held in 2012 were actually two separate events: the Winter Olympics weren't held in 2012, but that's a story for another time.\n",
            "\n",
            "I'm assuming you're referring to the Summer Olympics. The 2012 Summer Olympics, formally known as Games of the XXX Olympiad, took place from July 27 to August 12, 2012. These Olympic Games were held in London, United Kingdom.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from pydantic_ai import Agent, RunContext, UnexpectedModelBehavior\n",
        "\n",
        "\n",
        "class DatabaseConn:\n",
        "    \"\"\"This is a fake database for example purposes.\n",
        "\n",
        "    In reality, you'd be connecting to an external database\n",
        "    (e.g. PostgreSQL) to get information about customers.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    async def customer_name(cls, *, id: int) -> str | None:\n",
        "        if id == 123:\n",
        "            return 'John'\n",
        "\n",
        "    @classmethod\n",
        "    async def customer_balance(cls, *, id: int, include_pending: bool) -> float:\n",
        "        if id == 123:\n",
        "            return 123.45\n",
        "        else:\n",
        "            raise ValueError('Customer not found')\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SupportDependencies:\n",
        "    customer_id: int\n",
        "    db: DatabaseConn\n",
        "\n",
        "\n",
        "class SupportResult(BaseModel):\n",
        "    support_advice: str = Field(description='Advice returned to the customer')\n",
        "    block_card: bool = Field(description='Whether to block their card')\n",
        "    risk: int = Field(description='Risk level of query', ge=0, le=10)\n",
        "\n",
        "\n",
        "support_agent = Agent(\n",
        "    'ollama:llama3.2',\n",
        "    deps_type=SupportDependencies,\n",
        "    result_type=SupportResult,\n",
        "    retries=20,\n",
        "    system_prompt=(\n",
        "        'You are a support agent in our bank, give the '\n",
        "        'customer support and judge the risk level of their query. '\n",
        "        \"Reply using the customer's name.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "@support_agent.system_prompt\n",
        "async def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n",
        "    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n",
        "    return f\"The customer's name is {customer_name!r}\"\n",
        "\n",
        "\n",
        "@support_agent.tool\n",
        "async def customer_balance(\n",
        "    ctx: RunContext[SupportDependencies], include_pending: bool\n",
        ") -> str:\n",
        "    \"\"\"Returns the customer's current account balance.\"\"\"\n",
        "    balance = await ctx.deps.db.customer_balance(\n",
        "        id=ctx.deps.customer_id,\n",
        "        include_pending=include_pending,\n",
        "    )\n",
        "    return f'${balance:.2f}'\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n",
        "    try:\n",
        "      result = await support_agent.run('What is my name and balance?', deps=deps)\n",
        "      print(result.data)\n",
        "    except UnexpectedModelBehavior:\n",
        "      print('Error')\n",
        "      raise\n",
        "\n",
        "    try:\n",
        "      result = await support_agent.run('I just lost my card!', deps=deps)\n",
        "      print(result.data)\n",
        "    except UnexpectedModelBehavior:\n",
        "      print('Error')\n",
        "      raise\n"
      ],
      "metadata": {
        "id": "DRXggOhFf4iC",
        "outputId": "f70ed5b1-da98-4f23-f8c9-88f913556be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnexpectedModelBehavior",
          "evalue": "Exceeded maximum retries (10) for result validation",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnexpectedModelBehavior\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-dbdd57036e75>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mdeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSupportDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatabaseConn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0msupport_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'What is my name and balance?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnexpectedModelBehavior\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/agent.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, user_prompt, message_history, model, deps, model_settings, usage_limits, usage, result_type, infer_name)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0m_logfire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'handle model response'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle_span\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                     final_result, tool_responses = await self._handle_model_response(\n\u001b[0m\u001b[1;32m    312\u001b[0m                         \u001b[0mmodel_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/agent.py\u001b[0m in \u001b[0;36m_handle_model_response\u001b[0;34m(self, model_response, run_context, result_schema)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# and a tool call response, where the text response just indicates the tool call will happen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtool_calls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_structured_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/agent.py\u001b[0m in \u001b[0;36m_handle_structured_response\u001b[0;34m(self, tool_calls, run_context, result_schema)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_messages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryPromptPart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_incr_result_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic_ai/agent.py\u001b[0m in \u001b[0;36m_incr_result_retry\u001b[0;34m(self, run_context)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0mrun_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_result_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             raise exceptions.UnexpectedModelBehavior(\n\u001b[0m\u001b[1;32m   1301\u001b[0m                 \u001b[0;34mf'Exceeded maximum retries ({self._max_result_retries}) for result validation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m             )\n",
            "\u001b[0;31mUnexpectedModelBehavior\u001b[0m: Exceeded maximum retries (10) for result validation"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}